qgen_model_name: "Mistral7B"
model_name: "mistralai/Mistral-7B-Instruct-v0.2"
questiongen_inference_params:
  max_new_tokens: 200
  temperature: 0.5
  do_sample: True
questiongen_prompts_file: "/net/nfs.cirrascale/mosaic/tejass/data/instruction_prompts/recoverr_prompts/questiongen_prompts_recoverr_verifyingevidences-verifying_prompt_detailed.json"